---
dg-publish: true
date_created: 2025-10-22T02:17:35-06:00
date_modified: 2025-10-27T00:18:45-06:00
dg-path: 01-Fundamentos-IA-Generativa.md
cover: "![[Gemini_Generated_Image_byudq9byudq9byud.png]]"
tags:
  - proyecto-ia
resumen: Define qu√© es la ingenier√≠a de prompts y sus t√©cnicas principales para guiar a los modelos.
icono: üí°
estado: Borrador
---

# Cap√≠tulo 1: Fundamentos de la IA Generativa y Grandes Modelos de Lenguaje

---

Para navegar con confianza el nuevo panorama de la investigaci√≥n acad√©mica asistida por la Inteligencia Artificial, es crucial comenzar por el principio: _entender qu√© es esta tecnolog√≠a, c√≥mo funciona y, sobre todo, c√≥mo podemos comunicarnos eficazmente con ella_. Este cap√≠tulo sienta las bases conceptuales y pr√°cticas que nos permitir√°n construir un flujo de trabajo s√≥lido y eficiente.

No pretendemos realizar una inmersi√≥n t√©cnica profunda, sino ofrecer un marco de referencia claro y funcional para el acad√©mico o acad√©mica. Abordaremos desde la definici√≥n de la [[09-Glosario#IA Generativa|IA Generativa]] o de prop√≥sito general y los modelos que la impulsan, hasta los conceptos fundamentales que determinan sus capacidades y limitaciones. Comprender estos fundamentos es esencial para aprovechar al m√°ximo estas herramientas y reconocer cu√°ndo y c√≥mo aplicarlas en nuestros proyectos de investigaci√≥n.

---

## 1.1 El Ascenso de la IA de Prop√≥sito General

Las capacidades de la Inteligencia Artificial moderna, tambi√©n conocida como **IA Generativa** o **[[09-Glosario#IA Generativa|IA de Prop√≥sito General]]**, han avanzado de forma sorprendente durante los √∫ltimos a√±os y han mostrado progresos particularmente notables en los √∫ltimos meses. En diversas pruebas de referencia estandarizadas (_benchmarks_), las cuales consisten en una serie de acertijos acad√©micos o desaf√≠os de ingenier√≠a de software, los modelos actuales de IA ya logran resolver una gran proporci√≥n de los problemas presentados, **alcanzando m√°s del 80%** en algunos casos recientes.

> [!INFO]- Puntuaciones en Benchmarks Clave 
> Los modelos m√°s avanzados han demostrado desempe√±o sobresaliente en m√∫ltiples √°reas:
> 
> - üî¥ **FrontierMath**: Matem√°ticas avanzadas
> - üü† **ARC-AGI**: Razonamiento abstracto (evaluaci√≥n semi-secreta)
> - üü¢ **SWE-bench**: Ingenier√≠a de software del mundo real
> - üîµ **GPQA**: Ciencia a nivel de posgrado
> - üü£ **AIME 2024**: Competencia matem√°tica para estudiantes de √©lite

Adem√°s, en pruebas aplicadas en diferentes √°reas como el procesamiento del lenguaje natural, la visi√≥n por computadora, el reconocimiento de voz y las matem√°ticas, han alcanzado e incluso superado el desempe√±o humano. Por ejemplo, apenas unas semanas atr√°s, los modelos m√°s avanzados obtuvieron un rendimiento de medalla de oro en la Olimpiada Internacional de Matem√°ticas 2025 [[08-Referencias#(DeepMind, 2025)|(DeepMind, 2025)]].

> [!WARNING] Cautela en las Comparaciones 
> Como ha se√±alado el matem√°tico Terence Tao, esto debe manejarse con cautela, ya que sin una metodolog√≠a de evaluaci√≥n controlada y previa para ambos bandos, hay que desconfiar de las comparaciones simplistas entre el desempe√±o de los modelos de IA en competiciones como la IMO y el de los concursantes humanos, pues los sistemas pueden beneficiarse de recursos (m√°s tiempo de c√≥mputo, reformulaci√≥n de enunciados, selecci√≥n de las mejores salidas, etc.) que no est√°n disponibles para los estudiantes [[08-Referencias#(Tao, 2025)|(Tao, 2025)]].

### ¬øPor qu√© este progreso acelerado?

Estas mejoras tienen principalmente dos razones fundamentales:

**1. Escalamiento:** Ahora se usan computadoras mucho m√°s potentes y se le proporciona mucha m√°s informaci√≥n para aprender. Es como si se le diera un "cerebro m√°s grande" y m√°s libros (m√°s im√°genes, audios o videos) para estudiar. Este fen√≥meno de _scaling_ ha demostrado que simplemente aumentar los recursos computacionales y los datos de entrenamiento produce mejoras consistentes en el desempe√±o.

**2. Mejores M√©todos de Razonamiento:** Se han desarrollado t√©cnicas m√°s sofisticadas para resolver problemas complejos:

- **Cadena de pensamiento (_[[09-Glosario#Cadena de Pensamiento|Chain of Thought]]_)**: La IA puede ahora dividir un desaf√≠o complejo en peque√±os pasos, pensando cuidadosamente uno por uno antes de responder. Es como cuando resuelves un problema de matem√°ticas paso a paso, "pensando en voz alta" y mostrando tu razonamiento gradualmente.
- **Aprendizaje por refuerzo**: Mediante un m√©todo similar al ensayo y error, el modelo prueba diferentes soluciones (a veces muy detalladas) y aprende a preferir aquellas que conducen a respuestas correctas.

### Capacidades Sorprendentes en 2025

Gracias a estos avances t√©cnicos, la IA de prop√≥sito general puede hacer cosas que hasta hace poco parec√≠an de ciencia ficci√≥n. Hace cinco a√±os, los principales modelos de IA de prop√≥sito general rara vez pod√≠an producir un p√°rrafo coherente de texto. Hoy podemos:

- Mantener **conversaciones naturales** con ella
- Pedirle que escriba **programas de ingenier√≠a de software** de tama√±o peque√±o a mediano
- Solicitar **traducciones precisas** entre varios idiomas
- Resolver **problemas de libros de texto** de matem√°ticas y ciencias a nivel de posgrado
- Encontrar y **resumir informaci√≥n relevante** en documentos extensos
- **Generar im√°genes** a partir de texto que son dif√≠ciles de distinguir de fotograf√≠as reales
- **Generar v√≠deos** cortos
- **Describir o transcribir** lo que observa en fotos o videos
- Trabajar **simult√°neamente con m√∫ltiples entradas**: texto, im√°genes, audio y video

> [!SUCCESS] Adopci√≥n Masiva 
> El impacto y la rapidez con que estas herramientas han llegado al p√∫blico son notables. Un ejemplo es ChatGPT de OpenAI: tras su lanzamiento a finales de 2022, consigui√≥ m√°s de un mill√≥n de usuarios en apenas cinco d√≠as y alcanz√≥ los 100 millones en solo dos meses. Esto lo convierte en una de las tecnolog√≠as que m√°s r√°pido se ha adoptado en la historia.

Esta nueva y sorprendente versatilidad ha provocado que la IA se integre r√°pidamente en nuestra vida diaria. La encontramos potenciando herramientas que usamos constantemente: desde los motores de b√∫squeda de Google [[08-Referencias#(Reid, 2024)|(Reid, 2024)]] y Microsoft [[08-Referencias#(The Bing Team, 2025)|(The Bing Team, 2025)]] hasta software creativo como Adobe Creative Cloud (Photoshop, Illustrator, Express) [[08-Referencias#(Adobe, 2024)|(Adobe, 2024)]], pasando por plataformas de asistencia legal y sistemas que ayudan a los m√©dicos a tomar decisiones.

Mirando hacia adelante, el potencial es a√∫n mayor. Entre las perspectivas m√°s prometedoras se encuentra su aplicaci√≥n para revolucionar la educaci√≥n haci√©ndola m√°s personalizada, para transformar la medicina con diagn√≥sticos m√°s precisos y el descubrimiento acelerado de f√°rmacos, y para impulsar la investigaci√≥n cient√≠fica en √°reas como la qu√≠mica, biolog√≠a y f√≠sica.

---

## 1.2 ¬øQu√© es la IA Generativa y los Grandes Modelos de Lenguaje?

La **IA generativa** se refiere a sistemas de inteligencia artificial capaces de _generar contenido nuevo_ (texto, im√°genes, audio, c√≥digo, etc.) a partir de patrones aprendidos, en lugar de solo realizar tareas de clasificaci√≥n o predicci√≥n. Un caso particular y fascinante son los **[[09-Glosario#Grandes Modelos de Lenguaje|Grandes Modelos de Lenguaje]]** ([[09-Glosario#LLM|LLM]], por sus siglas en ingl√©s), que son modelos de _aprendizaje profundo_ de gran escala entrenados con enormes cantidades de texto para producir lenguaje natural coherente [[08-Referencias#(Amazon Web Services, s. f.)|(Amazon Web Services, s. f.)]].

```mermaid
graph LR
    A[IA Generativa] --> B[Texto: LLMs]
    A --> C[Im√°genes: DALL-E, Midjourney]
    A --> D[Audio: M√∫sica, Voz]
    A --> E[Video: Generaci√≥n de clips]
    A --> F[C√≥digo: Copilot, CodeLlama]
    
    style B fill:#4A90E2
    style A fill:#E8F4F8
```

Estos modelos pueden responder preguntas, mantener conversaciones, traducir, programar y mucho m√°s, adapt√°ndose a multitud de tareas sin reentrenamiento espec√≠fico. En esencia, un LLM aprende la probabilidad de continuaci√≥n de secuencias de texto: **genera palabra por palabra (en realidad, _[[09-Glosario#Tokens|token]]_ por token) el contenido m√°s probable** dado un contexto previo.

### La Arquitectura Transformer: El Motor de los LLMs

Gracias a arquitecturas modernas, especialmente los _[[09-Glosario#Transformers|Transformers]]_, los LLM aprenden las relaciones entre palabras y conceptos en el lenguaje de forma no secuencial sino _paralela_, capturando dependencias de largo alcance en un texto [[08-Referencias#(Amazon Web Services, s. f.)|(Amazon Web Services, s. f.)]]. Esto les permite _entender el contexto global_ de una entrada y producir respuestas m√°s coherentes.

> [!TIP]+ Pre-entrenamiento y Ajuste Fino
> La mayor√≠a de LLM actuales son **modelos generativos pre-entrenados** (_Generative Pre-trained Transformers_, de ah√≠ nombres como GPT):
>
> 1. **[[09-Glosario#Pre-entrenamiento|Pre-entrenamiento]]**: Se entrenan en un corpus gigantesco de texto mediante aprendizaje autosupervisado (prediciendo la siguiente palabra)
> 2. **[[09-Glosario#Ajuste fino|Ajuste fino]]** (_fine-tuning_): Se refinan con t√©cnicas adicionales para tareas concretas o para seguir instrucciones humanas (p.ej. afinaci√≥n con refuerzo y feedback humano, _[[09-Glosario#RLHF|RLHF]]_)

Un aspecto clave es que los LLM son extremadamente flexibles: con _las mismas neuronas_ pueden redactar un ensayo, escribir c√≥digo o responder una pregunta factual simplemente cambiando la _instrucci√≥n o prompt_ que reciben. Esto representa un cambio de paradigma frente a modelos m√°s peque√±os o espec√≠ficos, y ha potenciado aplicaciones como asistentes virtuales avanzados, herramientas de resumen autom√°tico y generadores de c√≥digo natural [[08-Referencias#(Amazon Web Services, s. f.)|(Amazon Web Services, s. f.)]].

> [!WARNING] Limitaciones Importantes
> Los LLM tambi√©n presentan limitaciones significativas:
>
> - No siempre "entienden" como un humano
> - Pueden generar informaci√≥n incorrecta (fen√≥meno de _[[09-Glosario#Alucinaciones|alucinaci√≥n]]_)
> - Su rendimiento depende de la calidad y amplitud de su entrenamiento
> - Pueden reproducir sesgos presentes en sus datos de entrenamiento

---

## 1.3 Conceptos Indispensables: Tokens, Ventana de Contexto y Memoria del Modelo

Para comprender el funcionamiento de los LLM y c√≥mo interactuamos con ellos, debemos conocer conceptos fundamentales que a menudo se comparan con la **memoria humana**: la _memoria de trabajo_ (de corto plazo) del modelo versus la _memoria de entrenamiento_ (conocimiento de largo plazo). Estas nociones se materializan en la **[[09-Glosario#Ventana de Contexto|ventana de contexto]]** y los **par√°metros entrenados** del modelo, respectivamente.

### 1.3.1 Tokens: La Unidad B√°sica de Procesamiento

Los LLM no leen palabras individualmente sino que las descomponen en piezas m√°s peque√±as llamadas **_[[09-Glosario#Tokens|tokens]]_**. Un token puede ser una palabra entera, un fragmento de palabra o un s√≠mbolo de puntuaci√≥n.

> [!EXAMPLE]+ Ejemplo de Tokenizaci√≥n 
> La frase _"¬øC√≥mo est√°s?"_ en espa√±ol puede descomponerse en 5 tokens [[08-Referencias#(Robles, 2025)|(Robles, 2025)]]:
> 
> - `¬ø`
> - `C√≥mo`
> - `est`
> - `√°s`
> - `?`

Cada modelo tiene su forma de tokenizar el texto (OpenAI, Google, Meta, etc., usan algoritmos de tokenizaci√≥n distintos), pero en promedio **1 token equivale a aproximadamente 3-4 caracteres** (aunque var√≠a seg√∫n el idioma; en ingl√©s ~0.75 palabras por token, en espa√±ol un token suele representar algo menos que una palabra entera) [[08-Referencias#(Robles, 2025)|(Robles, 2025)]].

```mermaid
graph TD
    A[Texto: ¬øC√≥mo est√°s?] --> B[Tokenizaci√≥n]
    B --> C[Token 1: ¬ø]
    B --> D[Token 2: C√≥mo]
    B --> E[Token 3: est]
    B --> F[Token 4: √°s]
    B --> G[Token 5: ?]
    
    style A fill:#E8F4F8
    style B fill:#4A90E2,color:#fff
    style C fill:#98D8C8
    style D fill:#98D8C8
    style E fill:#98D8C8
    style F fill:#98D8C8
    style G fill:#98D8C8
```

Esto es importante porque tanto la entrada que proporcionamos al modelo como la salida que genera se miden en n√∫mero de tokens.

### 1.3.2 Ventana de Contexto: La Memoria de Trabajo

La **ventana de contexto** es la cantidad m√°xima de tokens que el modelo puede "tener en mente" o procesar a la vez en una interacci√≥n. Es an√°loga a la **memoria de corto plazo** humana: limita cu√°nta informaci√≥n previa el modelo puede recordar durante la generaci√≥n de una respuesta [[08-Referencias#(Robles, 2025)|(Robles, 2025)]].

> [!INFO]- ¬øQu√© Incluye la Ventana de Contexto? 
> En aplicaciones conversacionales, la ventana de contexto incluye:
> 
> - **Instrucciones del sistema**: Prompts ocultos que gu√≠an estilo y comportamiento
> - **Historial de la conversaci√≥n**: Todos los mensajes previos recientes
> - **Datos recuperados**: De bases de conocimiento (en enfoques RAG)
> - **Tu pregunta actual**: El nuevo mensaje del usuario
> - **La respuesta generada**: Los tokens que el modelo produce
> - **Marcas especiales de formato**: Delimitadores y metadatos
> 
> Todos estos elementos compiten por espacio en la ventana de contexto finita [[08-Referencias#(Robles, 2025)|(Robles, 2025)]].

Por ejemplo, un modelo con ventana de 4,000 tokens puede considerar unos 4K tokens combinando la pregunta del usuario y la respuesta que genere. Si se excede ese l√≠mite, el modelo no procesar√° el texto adicional (o lo truncar√°), resultando en p√©rdidas de informaci√≥n.

> [!EXAMPLE] Ejemplo Pr√°ctico 
> Esto explica por qu√© no podemos simplemente darle _el texto completo de "Don Quijote" a ChatGPT-3.5_ (con ~4K tokens de contexto) y pedir un resumen ‚Äî **superar√≠a su ventana y no podr√≠a ni leerlo** [[08-Referencias#(Codificando Bits, 2023)|(Codificando Bits, 2023)]].

Ha surgido la **ingenier√≠a de contexto** como disciplina: decidir estrat√©gicamente qu√© informaci√≥n incluir o resumir en el prompt para aprovechar al m√°ximo la ventana limitada.

### 1.3.3 Memoria de Entrenamiento: El Conocimiento a Largo Plazo

A diferencia de la memoria de trabajo acotada por la ventana de contexto, un LLM posee una suerte de _memoria impl√≠cita de largo plazo_ en sus **par√°metros entrenados**. Durante el pre-entrenamiento, el modelo ajust√≥ miles de millones (hasta _cientos de miles de millones_ en los LLM m√°s grandes) de par√°metros para absorber patrones del lenguaje y hechos del mundo a partir de textos masivos (libros, art√≠culos, p√°ginas web, c√≥digo, etc.).

Esto significa que _gran parte del conocimiento est√° almacenado en el modelo_ tras el entrenamiento, an√°logo a conocimientos que un humano almacena en su memoria a largo plazo. Por ejemplo, GPT-3 se entren√≥ con ~45 terabytes de texto y alcanz√≥ 175 mil millones de par√°metros, incorporando as√≠ enormes cantidades de informaci√≥n general (datos hasta ~2021) en sus "pesos" [[08-Referencias#(Amazon Web Services, s. f.)|(Amazon Web Services, s. f.)]].

```mermaid
graph TB
    A[Modelo de Lenguaje] --> B[Ventana de Contexto<br/>Memoria de Trabajo]
    A --> C[Par√°metros Entrenados<br/>Memoria de Largo Plazo]
    
    B --> D[Limitada: 4K - 2M tokens]
    B --> E[Informaci√≥n actual<br/>de la conversaci√≥n]
    B --> F[Se resetea cada sesi√≥n]
    
    C --> G[Masiva: Billones de par√°metros]
    C --> H[Conocimiento general<br/>hasta fecha de corte]
    C --> I[Permanente pero imperfecta]
    
    style A fill:#4A90E2,color:#fff
    style B fill:#98D8C8
    style C fill:#F5A623
    style D fill:#E8F4F8
    style E fill:#E8F4F8
    style F fill:#E8F4F8
    style G fill:#FEF5E7
    style H fill:#FEF5E7
    style I fill:#FEF5E7
```

> [!WARNING]+ No es una memoria perfecta 
> Esta memoria de entrenamiento **no es perfecta ni organizada**:
> 
> - Los LLM no recuerdan datos concretos como una base de datos
> - Generan respuestas basadas en patrones estad√≠sticos
> - A veces "recuerdan" con exactitud sorprendente (definiciones, c√≥digo conocido, citas)
> - Otras veces confabulan detalles incorrectos si no "memorizaron" ese dato con fidelidad

Investigaciones muestran que los transformadores pueden llegar a memorizar trozos exactos de sus datos de entrenamiento, especialmente aquellos √∫nicos o repetitivos [[08-Referencias#(Zia, 2025)|(Zia, 2025)]]. Esto plantea riesgos: **un modelo puede sin querer revelar informaci√≥n sensible** presente en sus datos de entrenamiento (p. ej., fragmentos de c√≥digo propietario o informaci√≥n personal) si se le induce correctamente [[08-Referencias#(Zia, 2025)|(Zia, 2025)]].

> [!TIP] Diferencia Clave 
> **Ventana de contexto** = Lo que el modelo puede ver y recordar _en este instante_  
> **Conocimiento entrenado** = Todo lo que _"ha aprendido" antes_ (hasta su fecha de corte de entrenamiento)

### ¬øPor qu√© importan tanto estos l√≠mites?

Porque determinan _lo que el modelo puede y no puede hacer_ con la informaci√≥n:

- Si nuestra pregunta requiere m√°s contexto del que cabe en su ventana, tendremos que **resumir o partir la tarea**
- Si preguntamos sobre hechos posteriores a la fecha de entrenamiento, el modelo posiblemente **no lo sepa** a menos que se lo proporcionemos en el prompt
- Muchos sistemas actuales **combinan LLMs con herramientas externas** (b√∫squedas en internet, bases de conocimiento actualizadas) para recuperar datos frescos

---

## 1.4 Funcionamiento General de los LLM: De la Entrada a la Salida

¬øC√≥mo logra un LLM producir una respuesta coherente a una pregunta? Comprender este proceso nos ayuda a formular mejores prompts y entender sus limitaciones.

```mermaid
flowchart TD
    A[Entrada del Usuario] --> B[1- Tokenizaci√≥n]
    B --> C[2- Embeddings<br/>Representaciones vectoriales]
    C --> D[3- Procesamiento Transformer<br/>Auto-atenci√≥n en m√∫ltiples capas]
    D --> E[4- Generaci√≥n Token por Token]
    E --> F{¬øCompletado?}
    F -->|No| G[Agregar token al contexto]
    G --> E
    F -->|S√≠| H[Respuesta Final]
    
    style A fill:#E8F4F8
    style B fill:#98D8C8
    style C fill:#98D8C8
    style D fill:#4A90E2,color:#fff
    style E fill:#F5A623
    style H fill:#98D8C8
```

### 1.4.1 Tokenizaci√≥n de la Entrada

El texto de entrada (prompt del usuario + contexto adicional como instrucciones del sistema o historial) se convierte en tokens num√©ricos. Gracias a las _incrustaciones_ ([[09-Glosario#Embeddings|embeddings]]), el modelo representa cada token como un vector en un espacio matem√°tico, de forma que tokens con significados relacionados quedan cercanos en ese espacio [[08-Referencias#(Amazon Web Services, s. f.)|(Amazon Web Services, s. f.)]]. Esto permite captar similitudes sem√°nticas m√°s all√° de coincidencias exactas de palabras.

### 1.4.2 Procesamiento mediante la Arquitectura Transformer

Los _Transformers_ revolucionaron los modelos de lenguaje al introducir el mecanismo de **auto-atenci√≥n**. En cada capa del modelo, cada token "presta atenci√≥n" a todos los dem√°s tokens en la secuencia para calcular qu√© tan relevantes son entre s√≠.

> [!EXAMPLE] Auto-atenci√≥n en Acci√≥n 
> En la frase "El perro persigue al gato porque **es** travieso", la auto-atenci√≥n adecuada ayuda a inferir que "es" se refiere al gato y no al perro.

Al procesar los tokens en _paralelo_ y con m√∫ltiples _cabezas de atenci√≥n_, el modelo construye representaciones internas ricas en contexto para cada posici√≥n de la secuencia. El gran tama√±o (muchas capas, muchas neuronas) le da capacidad de expresar funciones complejas del lenguaje.

### 1.4.3 Generaci√≥n Token por Token

Una vez procesada la entrada, el modelo produce un token de salida a la vez (cada nuevo token se incorpora al contexto y el proceso se repite). En cada paso escoge el token m√°s probable seg√∫n la distribuci√≥n aprendida, **aunque a menudo se introduce aleatoriedad controlada** (temperatura, top-k, etc.) para que las respuestas no sean siempre mec√°nicamente iguales. Este proceso contin√∫a hasta completar la respuesta o alcanzar el l√≠mite de la ventana de contexto.

> [!INFO] Importante Destacar 
> El modelo _no "entiende" en el sentido humano_, sino que sigue patrones estad√≠sticos. Sin embargo, la inmensa cantidad de datos y par√°metros le permiten _aproximar comprensi√≥n_ de manera funcional. Por ejemplo, un LLM puede traducir de un idioma a otro no porque sepa reglas gramaticales expl√≠citamente, sino porque ha inferido un inmenso mapa de correspondencias y usos del lenguaje durante su entrenamiento.

### 1.4.4 Ajuste Fino y Alineaci√≥n

Tras el pre-entrenamiento, muchos LLMs pasan por etapas adicionales de refinamiento:

- **Ajuste fino instructivo**: Se alimentan ejemplos de instrucciones humanas y respuestas deseadas
- **Aprendizaje por refuerzo con feedback humano (RLHF)**: Evaluadores humanos gu√≠an el modelo para que sus respuestas sean √∫tiles y seguras

Esto los convierte en asistentes m√°s alineados a lo que espera un usuario t√≠pico al interactuar (por ejemplo, rechazar solicitudes inapropiadas, seguir el tono pedido, etc.). Aun con estas t√©cnicas, los modelos pueden producir sesgos o contenidos objetables, lo que subraya que _siguen sin comprender plenamente las implicaciones de sus respuestas_ ‚Äì simplemente han ajustado sus probabilidades de salida.

---

## 1.5 Evoluci√≥n Reciente de los LLM: Modelos Destacados y Sus Capacidades en 2025

El campo de los LLM avanza r√°pidamente. A continuaci√≥n, presentamos una **comparativa funcional** de algunos de los modelos generativos m√°s recientes (2024-2025) y l√≠deres del mercado. El objetivo no es ser exhaustivo con cada detalle t√©cnico, sino dar un panorama de _hasta d√≥nde han llegado_ estos modelos en t√©rminos de capacidad, tama√±o, ventana de contexto y enfoques diferenciadores.

### 1.5.1 GPT-5 (OpenAI)

Lanzado por OpenAI en agosto de 2025, GPT-5 se presenta como _su modelo de lenguaje unificado m√°s avanzado hasta la fecha_ [[08-Referencias#(OpenAI, 2025)|(OpenAI, 2025)]].

**Caracter√≠sticas Principales:**

- **Razonamiento Modular**: Integra varios "modos" de razonamiento en un solo sistema, con un enrutador inteligente que decide si una consulta se responde de forma r√°pida con un sub-modelo eficiente o si activa un modo de _pensamiento profundo_ para preguntas complejas [[08-Referencias#(OpenAI, 2025)|(OpenAI, 2025)]].
    
- **Variantes Disponibles**:
    
    - **GPT-5 est√°ndar**: Para uso general
    - **GPT-5 Pro**: Para suscriptores, con capacidades de razonamiento extendido m√°s potentes
    - **GPT-5-mini y GPT-5-nano**: Para diferentes velocidades/costos [[08-Referencias#(Kargwal, 2025)|(Kargwal, 2025)]]
- **[[09-Glosario#Multimodalidad|Multimodalidad]] Nativa**: Habilidades de visi√≥n y voz integradas. Puede analizar im√°genes, diagramas, reconocer comandos de voz o generar respuestas habladas.
    

> [!SUCCESS]+ Ventana de Contexto Masiva 
> GPT-5 maneja hasta **~400,000 tokens de contexto** en ciertas configuraciones [[08-Referencias#(Cogni Down Under, 2025)|(Cogni Down Under, 2025)]], equivalente a cientos de p√°ginas de texto. Esto representa un salto enorme respecto a GPT-4 (8K-32K) y GPT-3.5 (4K).

**Mejoras en Fiabilidad:**

- Reducci√≥n significativa de alucinaciones
- Mayor fidelidad al seguir instrucciones
- Mejor desempe√±o en programaci√≥n, matem√°ticas avanzadas y redacci√≥n creativa [[08-Referencias#(OpenAI, 2025)|(OpenAI, 2025)]]

**Conocimiento actualizado hasta**: Finales de 2024

### 1.5.2 Claude 4.5 (Anthropic)

Claude es la familia de modelos desarrollada por Anthropic, un laboratorio enfocado en IA responsable. **Claude 4.5** (nombre c√≥digo _Claude Sonnet 4.5_) se posiciona como **el modelo puntero orientado a tareas complejas de razonamiento y coding** [[08-Referencias#(Anthropic, 2025)|(Anthropic, 2025)]].

**Caracter√≠sticas Principales:**

- **Excelencia en Codificaci√≥n**: Anthropic lo describe como "el mejor modelo de codificaci√≥n del mundo" en su lanzamiento, destacando su capacidad para manejar proyectos de programaci√≥n de larga duraci√≥n con m√≠nima intervenci√≥n humana [[08-Referencias#(Anthropic, 2025)|(Anthropic, 2025)]].
    
- **Agente Aut√≥nomo**: Especialmente bueno planificando y ejecutando secuencias de acciones (navegar herramientas, escribir y depurar c√≥digo, manejar m√∫ltiples pasos en tareas de investigaci√≥n) manteniendo contexto en horizontes amplios.
    
- **Enfoque en Seguridad**: Incorpora mejoras para evitar respuestas t√≥xicas o no √©ticas, siendo su modelo "m√°s alineado" hasta la fecha [[08-Referencias#(Anthropic, 2025)|(Anthropic, 2025)]].
    

**Ventana de Contexto:**

- **Oficial**: 200,000 tokens (equivalente a ~150,000 palabras)
- **Experimental**: Hasta 1 mill√≥n de tokens en entornos empresariales selectos [[08-Referencias#(Cogni Down Under, 2025)|(Cogni Down Under, 2025)]]

**Variantes Disponibles:**

|Variante|Prop√≥sito|Caracter√≠sticas|
|---|---|---|
|**Claude Sonnet 4.5**|Razonamiento intenso|Versi√≥n completa, ideal para tareas complejas|
|**Claude Haiku 4.5**|Velocidad y costo|Optimizada para respuesta en tiempo real, ofrecida gratuitamente por tiempo limitado [[08-Referencias#(Nu√±ez, 2025)|

> [!TIP] Fortaleza Principal 
> Claude 4.5 se destaca por **fiabilidad en tareas de larga duraci√≥n, seguridad en las respuestas y manejo de contextos masivos**. Ha liderado el ranking **SWE-bench** de tareas de programaci√≥n de extremo a extremo [[08-Referencias#(Anthropic, 2025)|(Anthropic, 2025)]].

**Multimodalidad**: Soporte visual b√°sico (procesamiento de im√°genes para asistir en programaci√≥n visual o lectura de documentos escaneados), pero su fortaleza principal sigue siendo texto y c√≥digo.

### 1.5.3 Gemini 2.5 (Google DeepMind)

Google, tras fusionar su equipo de investigaci√≥n Brain con DeepMind, introdujo la saga de modelos **Gemini**. **Gemini 2.5**, lanzado a inicios-mediados de 2025, se posiciona como _el modelo m√°s inteligente de Google hasta la fecha_ [[08-Referencias#(Google, 2025)|(Google, 2025)]].

**Caracter√≠sticas Principales:**

- **Modelos con Pensamiento**: Incorpora de forma nativa t√©cnicas de _chain-of-thought_. Gemini 2.5 puede "pensar" varios pasos internamente antes de dar una respuesta, emulando un razonamiento deliberativo [[08-Referencias#(Google, 2025)|(Google, 2025)]].

**Variantes Disponibles:**

|Variante|Capacidad|Ventana de Contexto|
|---|---|---|
|**Gemini 2.5 Pro**|M√°xima, para tareas complejas|1,000,000 tokens (planea extender a 2M)|
|**Gemini 2.5 Flash**|Optimizada para rapidez y escala|-|

> [!SUCCESS]+ Ventana de Contexto R√©cord 
> Gemini 2.5 Pro soporta hasta **1,000,000 de tokens de contexto**, equivalente a aproximadamente **1,500 p√°ginas de texto o 30,000 l√≠neas de c√≥digo** [[08-Referencias#(Google, 2025)|(Google, 2025)]]. Google planea extenderla a _2 millones_ en futuras actualizaciones [[08-Referencias#(Google, 2025)|(Google, 2025)]].

**Multimodalidad Completa**: Gemini es nativamente multimodal, aceptando _texto, im√°genes, audio e incluso video como entrada_. Puede analizar el contenido de una imagen, transcribir y analizar un audio, o describir un video corto en la misma sesi√≥n.

**Seguridad**: Incorpora avanzadas protecciones contra _prompt injections_ y contenido malicioso, apuntando al uso empresarial seguro [[08-Referencias#(Google, 2025)|(Google, 2025)]].

**Acceso**: Disponible a trav√©s de **Vertex AI** en Google Cloud y **Google AI Studio** para investigadores.

> [!WARNING] Desaf√≠os de Contexto Extremo 
> A pesar de su gran contexto, manejar coherencia en 1M de tokens es desafiante y a veces "olvida" partes remotas del contexto [[08-Referencias#(Cogni Down Under, 2025)|(Cogni Down Under, 2025)]].

### 1.5.4 Otros Modelos y Panorama General

El ecosistema de LLM en 2025 incluye otros actores notables que ampl√≠an las opciones disponibles para investigadores:

**LLaMA 2 y el Ecosistema Open Source (Meta)**

Meta abri√≥ en 2023 la puerta de los _LLM de c√≥digo abierto_ con **LLaMA 2**, un modelo de hasta 70 mil millones de par√°metros disponible para uso tanto investigador como comercial [[08-Referencias#(Bergmann, 2023)|(Bergmann, 2023)]]. Caracter√≠sticas:

- Ventana de contexto: 4K tokens (doble que LLaMA 1)
- Demostr√≥ que comunidades abiertas pueden afinar y mejorar modelos sustanciales
- Inspir√≥ multitud de variantes open-source: Mistral 7B, Falcon, etc.

**Otros Jugadores Emergentes:**

- **xAI Grok**: Conectado en tiempo real a la web para respuestas con informaci√≥n actualizada
- **Amazon Bedrock**: Plataforma que ofrece acceso a Claude, Titan (propio de AWS) y otros modelos
- **DeepSeek**: Modelos open-source con enfoque en eficiencia

```mermaid
graph TB
    A[Modelos LLM 2025] --> B[Propietarios<br/>Premium]
    A --> C[Open Source<br/>Comunidad]
    
    B --> D[GPT-5<br/>OpenAI]
    B --> E[Claude 4.5<br/>Anthropic]
    B --> F[Gemini 2.5<br/>Google]
    
    C --> G[LLaMA 2<br/>Meta]
    C --> H[Mistral<br/>7B-8x22B]
    C --> I[Falcon<br/>UAE]
    
    style A fill:#4A90E2,color:#fff
    style B fill:#F5A623
    style C fill:#98D8C8
```

> [!INFO]+ Tama√±os y Eficiencia 
> En cuanto a _par√°metros_, los tama√±os siguen en aumento pero con matices:
> 
> - GPT-5 se rumorea que puede tener del orden del trill√≥n de par√°metros (no confirmado)
> - Claude 4.5 no revel√≥ su tama√±o exacto, se asume en cientos de miles de millones
> - Gemini 2.5 Pro est√° en la liga de los cientos de miles de millones
> 
> Sin embargo, la eficacia no es solo cuesti√≥n de cantidad de par√°metros, sino de _calidad de entrenamiento_ y _novedades arquitect√≥nicas_. Modelos abiertos m√°s peque√±os (6B-20B par√°metros) finamente ajustados pueden superar a modelos enormes gen√©ricos en tareas particulares.

### 1.5.5 Tabla Comparativa Resumida

| Modelo                | Organizaci√≥n    | Ventana de Contexto        | Fortalezas                                       | Multimodal                         |
| --------------------- | --------------- | -------------------------- | ------------------------------------------------ | ---------------------------------- |
| **GPT-5**             | OpenAI          | ~400K tokens               | Razonamiento modular, reducci√≥n de alucinaciones | ‚úÖ (visi√≥n + voz)                   |
| **Claude 4.5 Sonnet** | Anthropic       | 200K (1M experimental)     | Codificaci√≥n, seguridad, contexto largo          | ‚ö†Ô∏è (b√°sico visual)                 |
| **Gemini 2.5 Pro**    | Google DeepMind | 1M tokens (2M planificado) | Pensamiento estructurado, contexto masivo        | ‚úÖ (texto + imagen + audio + video) |
| **LLaMA 2**           | Meta            | 4K tokens                  | Open source, personalizable                      | ‚ùå                                  |

---

## 1.6 Conclusi√≥n: Fundamentos para la Investigaci√≥n Acad√©mica

Los fundamentos de los LLM abarcan entender c√≥mo manejan contexto y conocimiento, y apreciar las diferencias entre las principales ofertas actuales. Hemos visto que:

1. **Conceptos clave** como la ventana de contexto definen la memoria activa del modelo, mientras que el entrenamiento define su memoria duradera.
    
2. **Modelos l√≠deres** como GPT-5, Claude 4.5 y Gemini 2.5 empujan los l√≠mites en diferentes direcciones:
    - Unificaci√≥n de modos de razonamiento
    - Contextos masivos para procesamiento de documentos extensos
    - Multimodalidad profunda para an√°lisis de diferentes tipos de datos
    - Especializaci√≥n en codificaci√≥n y agentes aut√≥nomos
3. **Limitaciones inherentes**: A pesar de sus impresionantes capacidades, estos modelos a√∫n no "comprenden" como humanos, pueden alucinar informaci√≥n y requieren prompts cuidadosamente dise√±ados.
    

Esta base conceptual nos servir√° para entender _por qu√©_ ciertas t√©cnicas funcionan y _c√≥mo_ sacar el mayor provecho de la IA generativa en nuestros workflows cient√≠ficos. A lo largo de este libro exploraremos:

- **Ingenier√≠a de Prompts** (Cap√≠tulo 2): C√≥mo _formular mejores instrucciones_ para obtener respuestas precisas y relevantes
- **Ingenier√≠a de Contexto** (Cap√≠tulo 3): C√≥mo _estructurar contextos_ y gestionar informaci√≥n dentro de la ventana limitada
- **Herramientas especializadas** (Cap√≠tulos 4-6): C√≥mo combinar modelos con fuentes de informaci√≥n y flujos de trabajo integrados
- **Consideraciones √©ticas** (Cap√≠tulo 7): Una mirada cr√≠tica a las implicaciones del uso de IA en investigaci√≥n

> [!TIP] Pr√≥ximo Paso 
> Ahora que comprendemos los fundamentos t√©cnicos, estamos listos para aprender el arte de comunicarnos eficazmente con estos sistemas. En el siguiente cap√≠tulo abordaremos la **Ingenier√≠a de Prompts**: la habilidad clave para transformar nuestras ideas en resultados precisos mediante instrucciones bien dise√±adas.

---